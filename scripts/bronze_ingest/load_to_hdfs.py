#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script: bronze_ingest/load_to_hdfs.py
PropÃ³sito: Cargar datos crudos desde CSV a formato Parquet (capa Bronze)
"""

from pyspark.sql import SparkSession
import os
import sys

def main():
    print("="*60)
    print("ğŸš€ INICIANDO INGESTA A CAPA BRONZE")
    print("="*60)
    
    # 1. Crear sesiÃ³n de Spark
    print("\nğŸ“Š Creando sesiÃ³n de Spark...")
    spark = SparkSession.builder \
        .appName("Bronze_Ingest_Ecommerce") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .getOrCreate()
    
    # 2. Ruta del CSV (ya verificada)
    ruta_csv = "/mnt/c/Users/Usuario/Downloads/Ecommerce_Sales_Prediction_Dataset.csv"
    
    # 3. Verificar que el CSV existe
    print(f"\nğŸ” Verificando archivo CSV: {ruta_csv}")
    
    if not os.path.exists(ruta_csv):
        print(f"âŒ ERROR: No se encuentra el archivo")
        sys.exit(1)
    
    print(f"âœ… Archivo encontrado: {ruta_csv}")
    
    # 4. Leer CSV con Spark
    print("\nğŸ“‚ Leyendo archivo CSV...")
    try:
        df = spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(f"file://{ruta_csv}")
        
        print(f"âœ… CSV cargado exitosamente")
        
    except Exception as e:
        print(f"âŒ Error al leer el CSV: {str(e)}")
        sys.exit(1)
    
    # 5. Mostrar informaciÃ³n del dataset
    num_filas = df.count()
    num_columnas = len(df.columns)
    
    print(f"\nğŸ“Š ESTADÃSTICAS DEL DATASET:")
    print(f"   â€¢ Filas: {num_filas}")
    print(f"   â€¢ Columnas: {num_columnas}")
    
    print("\nğŸ“‹ Esquema de datos:")
    df.printSchema()
    
    print("\nğŸ‘€ Primeras 5 filas:")
    df.show(5, truncate=False)
    
    # 6. Guardar en LOCAL (capa Bronze)
    ruta_local = "/home/hadoop/topicos-sales-prediction/data/bronze/ecommerce"
    
    print(f"\nğŸ’¾ Guardando datos en LOCAL...")
    print(f"   Ruta: {ruta_local}")
    
    # Crear directorio si no existe
    os.makedirs(ruta_local, exist_ok=True)
    
    try:
        df.write \
            .mode("overwrite") \
            .format("parquet") \
            .option("compression", "snappy") \
            .save(f"file://{ruta_local}")
        
        print(f"âœ… Datos guardados exitosamente en LOCAL")
        
    except Exception as e:
        print(f"âŒ Error al guardar: {str(e)}")
        sys.exit(1)
    
    # 7. Verificar archivos guardados
    print("\nğŸ” Archivos guardados:")
    os.system(f"ls -la {ruta_local}")
    
    # 8. Resumen final
    print("\n" + "="*60)
    print("âœ… INGESTA A BRONZE COMPLETADA EXITOSAMENTE")
    print("="*60)
    print(f"\nğŸ“ Datos guardados en: {ruta_local}")
    print(f"ğŸ“Š Total de registros: {num_filas}")
    print(f"ğŸ“‹ Columnas disponibles: {num_columnas}")
    
    # 9. Cerrar sesiÃ³n
    spark.stop()
    print("\nğŸ‘‹ SesiÃ³n de Spark cerrada")

if __name__ == "__main__":
    main()